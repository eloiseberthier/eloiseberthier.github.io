# jemdoc: menu{./MENU}{ressources.html}
# addjs{./jquery}
= Research

~~~

== Publications and Preprints

A more up-to-date list might be found on my [https://scholar.google.fr/citations?user=-PQBEZMAAAAJ&hl=fr Google Scholar].

- P. Mangold, E. Berthier, E. Moulines. *Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents*. /preprint/, 2025. \n\[[https://arxiv.org/pdf/2512.17688 arxiv]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.
\<\/div\>

- A. Groudiev, F. Schramm, E. Berthier, J. Carpentier, F. Dümbgen. *Sampling-Based Global Optimal Control and Estimation via Semidefinite Programming*. /preprint/, 2025. \n\[[https://arxiv.org/pdf/2507.17572 arxiv]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* Global optimization has gained attraction over the past decades, thanks to the development of both theoretical foundations and efficient numerical routines. Among recent advances, Kernel Sum of Squares (KernelSOS) provides a powerful theoretical framework, combining the expressivity of kernel methods with the guarantees of SOS optimization. In this paper, we take KernelSOS from theory to practice and demonstrate its use on challenging control and robotics problems. We identify and address the practical considerations required to make the method work in applied settings: restarting strategies, systematic calibration of hyperparameters, methods for recovering minimizers, and the combination with fast local solvers. As a proof of concept, the application of KernelSOS to robot localization highlights its competitiveness with existing SOS approaches that rely on heuristics and handcrafted reformulations to render the problem polynomial. Even in the high-dimensional, non-parametric setting of trajectory optimization with simulators treated as black boxes, we demonstrate how KernelSOS can be combined with fast local solvers to uncover higher-quality solutions without compromising overall runtimes.
\<\/div\>

- E. Mauduit, E. Berthier, A. Simonetto. *No-Regret Gaussian Process Optimization of Time-Varying Functions*. /preprint/, 2025. \n\[[https://arxiv.org/pdf/2512.00517 arxiv]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* Sequential optimization of black-box functions from noisy evaluations has been widely studied, with Gaussian Process bandit algorithms such as GP-UCB guaranteeing no-regret in stationary settings. However, for time-varying objectives, it is known that no-regret is unattainable under pure bandit feedback unless strong and often unrealistic assumptions are imposed. In this article, we propose a novel method to optimize time-varying rewards in the frequentist setting, where the objective has bounded RKHS norm. Time variations are captured through uncertainty injection (UI), which enables heteroscedastic GP regression that adapts past observations to the current time step. As no-regret is unattainable in general in the strict bandit setting, we relax the latter allowing additional queries on previously observed points. Building on sparse inference and the effect of UI on regret, we propose \textbf{W-SparQ-GP-UCB}, an online algorithm that achieves no-regret with only a vanishing number of additional queries per iteration. To assess the theoretical limits of this approach, we establish a lower bound on the number of additional queries required for no-regret, proving the efficiency of our method. Finally, we provide a comprehensive analysis linking the degree of time-variation of the function to achievable regret rates, together with upper and lower bounds on the number of additional queries needed in each regime.
\<\/div\>

- E. Mauduit, E. Berthier, A. Simonetto. *Time-varying Gaussian Process Bandit Optimization with Experts: no-regret in logarithmically-many side queries*. /ECML-PKDD/, 2025. \n\[[https://arxiv.org/pdf/2510.21274 arxiv]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* We study a time-varying Bayesian optimization problem with bandit feedback, where the reward function belongs to a Reproducing Kernel Hilbert Space (RKHS). We approach the problem via an upperconfidence bound Gaussian Process algorithm, which has been proven to
yield no-regret in the stationary case. The time-varying case is more challenging and no-regret results are out of reach in general in the standard setting. As such, we instead tackle the question of how many additional observations asked to an expert are required to regain a no-regret property. To do so, we formulate the presence of past observation via an uncertainty injection procedure, and we reframe the problem as a heteroscedastic Gaussian Process regression. In addition, to achieve a no-regret result, we discard long outdated observations and replace them with updated (possibly very noisy) ones obtained by asking queries to an external expert. By leveraging and extending sparse inference to the heteroscedastic case, we are able to secure a no-regret result in a challenging time-varying setting with only logarithmically-many side queries per time step. Our method demonstrates that minimal additional information suffices to counteract temporal drift, ensuring efficient optimization despite time variation.
\<\/div\>

- S. Chorna, K. Tarelkina, E. Berthier, G. Franchi. *Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs*. /preprint/, 2025. \n\[[https://arxiv.org/pdf/2507.05810 arxiv]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* While concept-based interpretability methods have traditionally focused on local explanations of neural network predictions, we propose a novel framework and interactive tool that extends these methods into the domain of mechanistic interpretability. Our approach enables a global dissection of model behavior by analyzing how high-level semantic attributes (referred to as concepts) emerge, interact, and propagate through internal model components. Unlike prior work that isolates individual neurons or predictions, our framework systematically quantifies how semantic concepts are represented across layers, revealing latent circuits and information flow that underlie model decision-making. A key innovation is our visualization platform that we named BAGEL (for Bias Analysis with a Graph for global Explanation Layers), which presents these insights in a structured knowledge graph, allowing users to explore concept-class relationships, identify spurious correlations, and enhance model trustworthiness. Our framework is model-agnostic, scalable, and contributes to a deeper understanding of how deep learning models generalize (or fail to) in the presence of dataset biases. The demonstration is available at https://knowledge-graph-ui-4a7cb5.gitlab.io/.
\<\/div\>

- R. Kazmierczak, S. Azzolin, E. Berthier, G. Frehse, G. Franchi. *Enhancing Concept Localization in CLIP-based Concept Bottleneck Models*. /preprint/, 2025. \n\[[https://arxiv.org/pdf/2510.07115 arxiv]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* This paper addresses explainable AI (XAI) through the lens of Concept Bottleneck Models (CBMs) that do not require explicit concept annotations, relying instead on concepts extracted using CLIP in a zero-shot manner. We show that CLIP, which is central in these techniques, is prone to concept hallucination, incorrectly predicting the presence or absence of concepts within an image in scenarios used in numerous CBMs, hence undermining the faithfulness of explanations. To mitigate this issue, we introduce Concept Hallucination Inhibition via Localized Interpretability (CHILI), a technique that disentangles image embeddings and localizes pixels corresponding to target concepts. Furthermore, our approach supports the generation of saliency-based explanations that are more interpretable.
\<\/div\>

- R. Kazmierczak, S. Azzolin, E. Berthier, A. Hedström, P. Delhomme, N. Bousquet, G. Frehse, M. Mancini, B. Caramiaux, A. Passerini, G. Franchi. *Benchmarking XAI Explanations with Human-Aligned Evaluations*. /AAAI AI Alignment Track/, 2026. \n\[[https://arxiv.org/pdf/2411.02470 arxiv]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* In this paper, we introduce PASTA (Perceptual Assessment System for explanaTion of Artificial intelligence), a novel framework for a human-centric evaluation of XAI techniques in computer vision. Our first key contribution is a human evaluation of XAI explanations on four diverse datasets (COCO, Pascal Parts, Cats Dogs Cars, and MonumAI) which constitutes the first large-scale benchmark dataset for XAI, with annotations at both the image and concept levels. This dataset allows for robust evaluation and comparison across various XAI methods. Our second major contribution is a data-based metric for assessing the interpretability of explanations. It mimics human preferences, based on a database of human evaluations of explanations in the PASTA-dataset. With its dataset and metric, the PASTA framework provides consistent and reliable comparisons between XAI techniques, in a way that is scalable but still aligned with human evaluations. Additionally, our benchmark allows for comparisons between explanations across different modalities, an aspect previously unaddressed. Our findings indicate that humans tend to prefer saliency maps over other explanation types. Moreover, we provide evidence that human assessments show a low correlation with existing XAI metrics that are numerically simulated by probing the model.
\<\/div\>

- R. Kazmierczak, E. Berthier, G. Frehse, G. Franchi. *Explainability for Vision Foundation Models: A Survey*. /Information Fusion/, 2025. \n\[[https://arxiv.org/pdf/2501.12203 arxiv]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* As artificial intelligence systems become increasingly integrated into daily life, the field of explainability has gained significant attention. This trend is particularly driven by the complexity of modern AI models and their decision-making processes. The advent of foundation models, characterized by their extensive generalization capabilities and emergent uses, has further complicated this landscape. Foundation models occupy an ambiguous position in the explainability domain: their complexity makes them inherently challenging to interpret, yet they are increasingly leveraged as tools to construct explainable models. In this survey, we explore the intersection of foundation models and eXplainable AI (XAI) in the vision domain. We begin by compiling a comprehensive corpus of papers that bridge these fields. Next, we categorize these works based on their architectural characteristics. We then discuss the challenges faced by current research in integrating XAI within foundation models. Furthermore, we review common evaluation methodologies for these combined approaches. Finally, we present key observations and insights from our survey, offering directions for future research in this rapidly evolving field.
\<\/div\>

- R. Kazmierczak, E. Berthier, G. Frehse, G. Franchi. *CLIP-QDA: An explainable concept bottleneck model*. /Transactions on Machine Learning Research Journal (TMLR)/, 2024. \n\[[https://arxiv.org/pdf/2312.00110 arxiv]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* In this paper, we introduce an explainable algorithm designed from a multi-modal foundation model, that performs fast and explainable image classification. Drawing inspiration from CLIP-based Concept Bottleneck Models (CBMs), our method creates a latent space where each neuron is linked to a specific word. Observing that this latent space can be modeled with simple distributions, we use a Mixture of Gaussians (MoG) formalism to enhance the interpretability of this latent space. Then, we introduce CLIP-QDA, a classifier that only uses statistical values to infer labels from the concepts. In addition, this formalism allows for both local and global explanations. These explanations come from the inner design of our architecture, our work is part of a new family of greybox models, combining performances of opaque foundation models and the interpretability of transparent models. Our empirical findings show that in instances where the MoG assumption holds, CLIP-QDA achieves similar accuracy with state-of-the-art methods CBMs. Our explanations compete with existing XAI methods while being faster to compute.
\<\/div\>


- D. Brellmann, E. Berthier, D. Filliat, G. Frehse. *On double descent in reinforcement learning with LSTD and random features*. /The Twelfth International Conference on Learning Representations (ICLR)/, 2024. \n\[[https://arxiv.org/pdf/2310.05518 arxiv]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and l2-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive deterministic limits of both the empirical and the true Mean-Squared Bellman Error (MSBE) that feature correction terms responsible for the double descent. Correction terms vanish when the l2-regularization is increased or the number of unvisited states goes to zero. Numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.
\<\/div\>


- T. Berthier, E. Berthier. *Mesurer la (haute) intensité d’un combat*. \[/In French/\], /RDN 860/, 2023. \n\[[https://www.defnat.com/e-RDN/vue-article.php?carticle=23175 journal]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* Définir la haute intensité d’un combat n’est pas chose aisée. Il peut y avoir le ressenti du combattant mais aussi la prise en compte de données factuelles. Celles-ci vont être combinées avec des facteurs de variabilité, permettant ainsi de quantifier cette notion d’intensité. Cette approche mathématique est aujourd’hui nécessaire au regard des évolutions.
\<\/div\>

- E. Berthier. *Efficient algorithms for control and reinforcement learning*. \[/PhD Thesis/\], /ENS Paris/, PSL Research University, France, 2022. \n\[[https://eloiseberthier.github.io/thesis_updated.pdf manuscript]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* Reinforcement learning describes how an agent can learn to act in an unknown environment in order to maximize its reward in the long run. It has its origins in the field of optimal control, as well as in some works in psychology. The increase in computational power and the use of approximation methods such as neural networks have led to recent successes, in particular in the resolution of games, yet without systematically providing theoretical guarantees. As for the field of optimal control, for which a model of the environment is provided, it has known solid theoretical developments since the 1960s, with numerical tools that have proven useful in many industrial applications. Nevertheless, the numerical resolution of high dimensional nonlinear control problems, which are typically encountered in robotics, remains relatively open today. In this thesis, we develop and analyze efficient algorithms, when possible with theoretical guarantees, for control and reinforcement learning. We show that, even though they are formulated differently, these two problems are very similar. We first focus on the discretization of continuous state deterministic Markov decision processes, by adapting a method developed for continuous time control. Then we propose a method for fast estimation of stability regions applicable to imperfectly known high-dimensional dynamical systems. We then generalize an algorithm for solving control problems derived from polynomial optimization, to non-polynomial systems known through a finite number of observations. For this, we use a sum-of-squares representation of smooth positive functions from kernel methods. Finally, we analyze a classical algorithm in reinforcement learning, the temporal-difference learning algorithm, in its non-parametric version. In particular, we insist on the link between the temporal-difference learning algorithm and the stochastic gradient descent algorithm, for which many convergence results are known.
\<\/div\>


- E. Berthier, Z. Kobeissi, F. Bach. *A Non-asymptotic Analysis of Non-parametric Temporal-Difference Learning*.  /Advances in Neural Information Processing Systems (NeurIPS)/, 2022. \n\[[https://hal.archives-ouvertes.fr/hal-03672958 hal], [poster_neurips.pdf poster], [slides_neurips.pdf slides]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* Temporal-difference learning is a popular algorithm for policy evaluation. In this paper, we study the convergence of the regularized non-parametric TD(0) algorithm, in both the independent and Markovian observation settings. In particular, when TD is performed in a universal reproducing kernel Hilbert space (RKHS), we prove convergence of the averaged iterates to the optimal value function, even when it does not belong to the RKHS. We provide explicit convergence rates that depend on a source condition relating the regularity of the optimal value function to the RKHS. We illustrate this convergence numerically on a simple continuous-state Markov reward process. 
\<\/div\>

- E. Berthier, J. Carpentier, A. Rudi, F. Bach. *Infinite-Dimensional Sums-of-Squares for Optimal Control*. /Conference on Decision and Control (CDC)/, 2022. \n\[[https://hal.archives-ouvertes.fr/hal-03377120 hal], [Poster_SOS.pdf poster], [Slides_ksos_Limoges.pdf slides]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* We introduce an approximation method to solve an optimal control problem /via/ the Lagrange dual of its weak formulation. It is based on a sum-of-squares representation of the Hamiltonian, and extends a previous method from polynomial optimization to the generic case of smooth problems. Such a representation is infinite-dimensional and relies on a particular space of functions-a reproducing kernel Hilbert space-chosen to fit the structure of the control problem. After subsampling, it leads to a practical method that amounts to solving a semi-definite program. We illustrate our approach by a numerical application on a simple low-dimensional control problem. 
\<\/div\>

- E. Berthier, J. Carpentier, F. Bach. *Fast and Robust Stability Region Estimation for Nonlinear Dynamical Systems*. /2021 European Control Conference (ECC) /, 2021. \n\[[https://hal.archives-ouvertes.fr/hal-02984348 hal], [presentation_ecc.pdf slides]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* A linear quadratic regulator can stabilize a nonlinear dynamical system with a local feedback controller around a linearization point, while minimizing a given performance criteria. An important practical problem is to estimate the region of attraction of such a controller, that is, the region around this point where the controller is certified to be valid. This is especially important in the context of highly nonlinear dynamical systems. In this paper, we propose two stability certificates that are fast to compute and robust when the first, or second derivatives of the system dynamics are bounded. Associated with an efficient oracle to compute these bounds, this provides a simple stability region estimation algorithm compared to classic approaches of the state of the art. We experimentally validate that it can be applied to both polynomial and non-polynomial systems of various dimensions, including standard robotic systems, for estimating region of attractions around equilibrium points, as well as for trajectory tracking. 
\<\/div\>

- E. Berthier, F. Bach. *Max-Plus Linear Approximations for Deterministic Continuous-State Markov Decision Processes*. /IEEE Control Systems Letters/, 4(3):767-772, 2020. \n\[[https://hal.archives-ouvertes.fr/hal-02617479 hal], [https://doi.org/10.1109/LCSYS.2020.2973199 journal], [presentation_dag.pdf slides]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* We consider deterministic continuous-state Markov decision processes (MDPs). We apply a max-plus linear method to approximate the value function with a specific dictionary of functions that leads to an adequate state-discretization of the MDP. This is more efficient than a direct discretization of the state space, typically intractable in high dimension. We propose a simple strategy to adapt the discretization to a problem instance, thus mitigating the curse of dimensionality. We provide numerical examples showing that the method works well on simple MDPs.
\<\/div\>

- E. Berthier. *Protection des données d'entraînement pour l'apprentissage statistique* \[/In French/\], 2019, /Conférence Intelligence Artificielle et Défense/. \n\[[https://www.cesar-conference.org/wp-content/uploads/2019/10/s6_p1_21_1530.pdf pdf]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* Les modèles d'apprentissage statistique sont susceptibles d'exposer les données qui ont été utilisées lors de leur entraînement. Ce
phénomène doit être pris en compte pour qualifier le niveau de sensibilité d'un modèle. La notion de confidentialité différentielle, créée à
l'origine pour la protection de la vie privée, répond partiellement à cette
problématique. En particulier, il est possible d'adapter le processus d'apprentissage de façon à vérifier certaines propriétés de confidentialité.
Lorsque les données sensibles sont distribuées sur plusieurs machines,
des processus cryptographiques permettent d’entraîner conjointement un
modèle sans en partager les données d’entraînement.
\<\/div\>


- E. Berthier. *Differential Privacy for Machine Learning* \[/Master's Thesis/\], 2019, EPFL, Lausanne, Switzerland. \n\[[master-thesis.pdf pdf], [poster.pdf poster]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* Machine learning algorithms can leak private information contained
in particular training data. Differential privacy ensures that an algorithm does not rely too strongly on any individual data point. Differentially private machine learning can be achieved by injecting noise in
the training process. In particular, the privacy of DP-SGD has already
been well-studied, yet only in the case where each training example
is sampled with replacement. We focus on the more practical case of
sampling without replacement, or shuffling, and try to provide privacy
guarantees for this algorithm. We also explore possible relaxations of
differential privacy.
\<\/div\>

- O. Kempf, E. Berthier. *IA, explicabilité et défense* \[/In French/\], 2019, /RDN 820 - L'Intelligence artificielle et ses enjeux pour la Défense/. \n\[[https://www.defnat.com/e-RDN/vue-article.php?carticle=22042 journal], [https://fr.calameo.com/books/0005581152ded50db4afd synopsis]\] \<span class=\"toggle-trigger\"\>\[Show Abstract\]\<span\>
\<div class=\"toggle-wrap\"\>
*Abstract:* L’IA est une réalité déjà ancienne mais son champ d’emploi ne cesse de s’élargir et accapare des domaines nouveaux, en particulier pour la défense. L’IA est polymorphe et se retrouve confrontée à un problème d’explicabilité. Pourquoi et comment sont les questions qui se posent pour les applications liées au contexte militaire.
AI is in itself old news but its fields of application never cease to expand and capture new ones, particularly in the defence domain. AI takes on many forms and faces a problem of how it should be described. Why? and how? are the questions to be asked about those applications with a military connection.
\<\/div\>

~~~


== PhD Students Supervision

- [https://theses.fr/s342704 Eliabelle Mauduit], co-supervised at ENSTA-UMA (2025-)

- [https://theses.fr/s352584 Rémi Kazmierczak], co-supervised at ENSTA-U2IS (2022-)

- [https://theses.fr/2024IPPAE008 David Brellmann], co-supervised at ENSTA-U2IS (2023-2024)


== Presentations & Outreach

-  On April 25th 2025, I was invited to present our ICLR 2024 paper on double descent in RL at Inria Scool's seminar in Lille.

-  On March 23th 2023, I was invited to present our CDC 2022 paper at Avignon Université.

-  On November 24th 2022, I have [slides_neurips.pdf presented] our NeurIPS 2022 paper at [https://neuripsinparis.github.io/neurips2022paris/ NeurIPS@Paris 2022].

-  I have defended my thesis on Thursday, October 27th 2022, at Inria Paris. You can download the [thesis_updated.pdf manuscript]. You can also have a look at the [slides_defense.pdf slides].

-  On June 21st 2022, I was a speaker at [https://www.rocq.inria.fr/semdoc/ Inria's Junior Seminar]. I talked about [20220621_Berthier.pdf reinforcement learning].

-  On June 17th 2022, I gave an [evian.pdf invited talk] at the [https://canum2020.math.cnrs.fr/accueil/ CANUM 2020] (45th Congrès National d'Analyse Numérique) in Evian. This was in a mini-symposium dedicated to numerical methods for Hamilton-Jacobi equations and mean-field games.

-  On June 3rd 2022, I gave a [Slides_ksos_Limoges.pdf talk] at the [https://indico.math.cnrs.fr/event/6564/ SMAI-MODE days] in Limoges. SMAI-MODE is the subgroup of the French Applied and Industrial Mathematics Society dedicated to optimization and decision.

-  On October 28th 2021, I gave a [CJC_MA_Controle_RL.pdf talk] at the [https://cjc-ma2021.github.io/ CJC-MA seminar], at Ecole polytechnique, Palaiseau. CJC-MA stands for ``Congrès des Jeunes Chercheuses et Chercheurs en Mathématiques Appliquées". It is its 1st edition, and will hopefully be organized each year in a different location by PhD students in applied mathematics.

-  On July 2nd 2021, I was invited to give a [presentation_dag.pdf talk] at the [https://cermics-lab.enpc.fr/seminaires/decision-algorithms-and-geometry/ Decision, Algorithms and Geometry seminar], a joint seminar between the optimization team of CERMICS (Ecole des Ponts) and the Tropical team (Inria Saclay and CMAP).

-  On July 1st 2021, I [presentation_ecc.pdf presented] a paper at the [https://ecc21.euca-ecc.org/ European Control Conference (ECC21)], virtually hosted in Rotterdam. 

-  In October 2020, with [https://clefourrier.github.io/ Clémentine Fourrier], we co-organized the [https://filles-et-maths.fr/rendez-vous-des-jeunes-mathematiciennes/ RJMI] (Rendez-vous des Jeunes Mathématiciennes et Informaticiennes), at Inria Paris, with the support of [https://animath.fr/ Animath]. This year's challenge was to set up a hybrid in-person & online event, which received great feedback from the participants!

-  From February 2020 to December 2021, we co-organized [https://www.rocq.inria.fr/semdoc/ Inria's Junior Seminar] with [https://merigoux.ovh/ Denis Merigoux], a monthly seminar which allows PhD students, interns & post-docs to present their work, through easily understandable talks, so that anyone can attend. This was an online - hybrid - in person event, depending on the circumstances.

-  Oral Presentation at [https://www.european-cyber-week.eu/ European Cyber Week], Rennes, November 2019.

-  In October 2019, I had the opportunity to co-organize the [https://filles-et-maths.fr/rendez-vous-des-jeunes-mathematiciennes/ RJMI] (Rendez-vous des Jeunes Mathématiciennes et Informaticiennes), at Inria Paris. During two days, a small group of female high school students are offered to meet researchers, attend research talks, and work on challenging math and computer science problems. This event is meant to promote scientific careers for women and prevent self-censoring. Stay tuned for next year's edition!

-  Poster Presentation at /Prairie Artificial Intelligence Summer School/ [https://project.inria.fr/paiss/paiss-2019/ \P.A.I.S.S.], Paris, October 2019.

-  Junior Organizing Commitee & Poster Presentation at /Paris-Saclay Junior Conference on Data Science and Engineering/ [https://jdse-paris.github.io/jDSE2019/ \#JSDE2019], Saclay, September 2019.

-  Outreach Presentation at [http://www.lyc-gaylussac.ac-limoges.fr/ Lycée Gay-Lussac], Limoges, March 2019. The idea was to introduce notions of data science and machine learning to high school and bachelor students. \[[hs.pdf high school slides], [bsc.pdf BSc slides]\]


== Miscellaneous

- Some research friends: [http://cermics.enpc.fr/~dalleg/ Guillaume Dalle], [https://pierremarion23.github.io/ Pierre Marion], [https://cmantoux.github.io/ Clément Mantoux].

- I'd recommend to adopt [https://github.com/Naereen/LaTeX-article-with-Pokemon-numbering this template] in research drafts.

